{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of HW2_Part2_distribute.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Surojit-KB/UNIV-AI-ML2-HW2/blob/main/Copy_of_HW2_Part2_distribute.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaA1H2AHyVWQ"
      },
      "source": [
        "# ML-2: Trees, Model Interrogation and Bayesian Workflow\n",
        "# Homework 2: Rossman Kaggle: Forecasting Sales\n",
        "# Part 2 : Modelling without embeddings!\n",
        "**ML-2 Cohort 1** <br>\n",
        "**Instructor: Dr. Rahul Dave**<br>\n",
        "**Max Score: 100** <br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jBFtovs3nh1"
      },
      "source": [
        "#importing libraries\n",
        "import numpy as np\n",
        "import scipy.stats\n",
        "import scipy.special\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.mlab as mlab\n",
        "import seaborn as sns\n",
        "from matplotlib import cm\n",
        "import pandas as pd\n",
        "from sklearn.pipeline import make_pipeline, make_union, Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "from keras.models import Sequential\n",
        "from keras.models import Model as KerasModel\n",
        "from keras.layers import Input, Dense, Activation, Reshape\n",
        "from keras.layers import Concatenate\n",
        "from keras.layers.embeddings import Embedding\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn import linear_model\n",
        "import pickle\n",
        "import csv\n",
        "from datetime import datetime\n",
        "from sklearn import preprocessing\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import xgboost as xgb\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.metrics import r2_score\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yI5GH62Ds-XK"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2sEs45v7Rr2"
      },
      "source": [
        "## Part 2: Modelling without Entity Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uk7vU2zkGSXG"
      },
      "source": [
        "Remember the parameters we need to use\n",
        "\n",
        "![Parameters.jpeg](https://drive.google.com/uc?export=view&id=1ROfqM3F5hWwJyrvQr_J1ATovNIW5niOs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5Hi8ehR7WFX"
      },
      "source": [
        "Lets import the feature_train_data.pickle file and set X,y values from the pickle file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVnBtoigYlt1",
        "outputId": "feab4f62-21d5-4ac9-df01-fc2d0637d5c2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arl_aj6X7VQk"
      },
      "source": [
        "with open(\"/content/drive/MyDrive/Colab Notebooks/UNIV-AI-ML2/HW/feature_train_data.pickle\", 'rb') as f:\n",
        "    (X,y) = pickle.load(f)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwMxtFTUDRFN"
      },
      "source": [
        "# we will split the train_ratio and 90% and 10% and set the train_size\n",
        "train_ratio = 0.9\n",
        "num_records = len(X)\n",
        "train_size = int(train_ratio * num_records)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpP1Emg2Dni0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd0fba6e-cf4e-48bb-d254-fd5358a9d645"
      },
      "source": [
        "#lets look at our data\n",
        "X[1], y[1]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([1058,    0,    0,    0,    1,    0,    0,    1]), '4491')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2fhYw49Dz3w"
      },
      "source": [
        "The next set of inputs is following:\n",
        "\n",
        "1. Do you want to one hot encode the data?\n",
        "2. Do you want to provide embeddings as input - this will be set to True for models with entity embeddings\n",
        "3. Do you want to save the emmbeddings? - again set to true if you want to entity embeddings\n",
        "4. if 3 is set to true, we want to save them to a embeddings.pickle\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tqtv6VWWDuZu"
      },
      "source": [
        "one_hot_as_input = True #one_hot is set to True\n",
        "embeddings_as_input = False #embeddings later on needs to set to true for Part 3\n",
        "save_embeddings = False\n",
        "saved_embeddings_fname = \"embeddings.pickle\"  # set save_embeddings to True to create this file"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAmd3posvMyz"
      },
      "source": [
        "### BASELINE MODEL WITHOUT ONE-HOT ENCODING \n",
        "TO UNDERSTAND HOW ONE-HOT ENCODING AFFECTS OUR RESULTS, I AM BUILDING A BASELINE RANDOM FOREST MODEL WITHOUT ANY ENCODING AND GETTING A BASELINE RESULTS. SINCE RANDOM FORESTS DO NOT REQUIRE THE VARIABLES TO BE ONE-HOT ENCODED, I EXCEPT IT TO PERFORM QUITE WELL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MD2mlwErpBc_"
      },
      "source": [
        "#WHILE SAVING THE DATA, WE HAD SAVED THE EARLIER DATA FIRST. HENCE WE CAN DIRECTLY SPLIT THE DATA ON THE BASIS OF INDICES\n",
        "#WE WOULD WANT TO TRAIN ON EARLIER DATA AND PREDICT ON LATER DATA, BECAUSE THAT IS HOW OUR PRACTICAL USE CASE WILL BE.\n",
        "X_train=X[:train_size,:]\n",
        "X_val=X[train_size:,:]\n",
        "y_train=y[:train_size]\n",
        "y_val=y[train_size:]\n",
        "y_train=np.log(y_train.astype(np.int))\n",
        "y_val=np.log(y_val.astype(np.int))\n",
        "y_max=max(np.max(y_train),np.max(y_val))\n",
        "y_train=y_train/y_max\n",
        "y_val=y_val/y_max"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StnmulRkEnRt"
      },
      "source": [
        "def sample(X, y, n):\n",
        "    '''random samples'''\n",
        "    num_row = X.shape[0]\n",
        "    indices = np.random.randint(num_row, size=n)\n",
        "    return X[indices, :], y[indices]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvXK__lwpdLY",
        "outputId": "1f5044ca-49a7-47ae-a6d4-fcdbee2dc60e"
      },
      "source": [
        "X_train, y_train = sample(X_train, y_train, 200000)  # Simulate data sparsity\n",
        "print(\"Number of samples used for training: \" + str(y_train.shape[0]))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of samples used for training: 200000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KT5efxrLE7Cd"
      },
      "source": [
        "**Lets define MAPE first**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRpKGtOLE6TV"
      },
      "source": [
        "#defining mape\n",
        "def MAPE(Y_actual,Y_Predicted):\n",
        "    #your code here\n",
        "    return np.mean(np.abs((Y_actual-Y_Predicted)/Y_actual))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3W-x3zTpm3W",
        "outputId": "8a3d32d2-79a6-4384-a31f-d8324b4df714"
      },
      "source": [
        "#your code here\n",
        "#MODEL WITHOUT ONE HOT ENCODING\n",
        "model=RandomForestRegressor(n_estimators = 200, max_depth = 35, min_samples_split = 2, min_samples_leaf = 1,n_jobs=-1,verbose=1)\n",
        "model.fit(X_train,y_train)\n",
        "y_pred=model.predict(X_val)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   12.0s\n",
            "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   50.2s\n",
            "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:   52.2s finished\n",
            "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.3s\n",
            "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    1.4s\n",
            "[Parallel(n_jobs=4)]: Done 200 out of 200 | elapsed:    1.5s finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GjBf-2VXptp3",
        "outputId": "2bb2a90e-b6cc-492c-dae3-7555c12938fc"
      },
      "source": [
        "print('MAPE score is:',MAPE(y_val,y_pred))\n",
        "print('R2 score is:',r2_score(y_val,y_pred))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MAPE score is: 0.016132731892635584\n",
            "R2 score is: 0.7786951516022276\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DqAfqVKvsMW"
      },
      "source": [
        "### MODELS WITH ONE HOT ENCODING\n",
        "HERE THERE ARE 2 PARTS:<BR>\n",
        "1. IN THE FIRST PART, I WILL ONE-HOT ENCODE ALL THE FEATURES EXCEPT THE STORE INDEX FEATURE. THE STORE-INDEX FEATURE HAS HIGH CARDINALITY AND I WANT TO SEE THE EFFECT ONE-HOT ENCODING HAS ON IT.\n",
        "2. IN THE SECOND PART, WE ARE GOING TO ONE-HOT ENCODE EVERYTHING AND OBSERVE THE RESULTS."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPSoah0_F9Ed"
      },
      "source": [
        "Define a function to one hot encode the training set and after split transform your training set using the function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KS03CrxWjcNN"
      },
      "source": [
        "#FUNCTION FOR ONE HOT ENCODING THE DATA\n",
        "#THE STORE INDEX FEATURE HAS 1115 CATEGORIES WHICH IS HIGH CARDINALITY\n",
        "#HENCE I AM FIRST TRYING TO MAKE A MODEL WHICH ONE-HOT ENCODES ALL THE OTHER FEATURES EXCEPT STORE INDEX\n",
        "#THIS IS JUST TO UNDERSTAND HOW THE ENCODING AFFECTS THE FEATURES \n",
        "def hot_encode(data):\n",
        "    #your code here\n",
        "    encoder=OneHotEncoder()\n",
        "    encoder.fit(data[:,1:])\n",
        "    train_data=encoder.transform(data[:,1:])\n",
        "    return train_data"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCk7tyVQEdy6"
      },
      "source": [
        "Split the data into X_train, X_val, y_train, y_val based on the train_size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olO5Mvzgs54_"
      },
      "source": [
        "train_data=hot_encode(X)\n",
        "train_data=np.column_stack((X[:,0].reshape(-1,1),train_data.todense()))"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLTrTSGPEaGw"
      },
      "source": [
        "#your code here\n",
        "#train_data=hot_encode(X)\n",
        "X_train=train_data[:train_size,:]\n",
        "X_val=train_data[train_size:,:]\n",
        "y_train=y[:train_size]\n",
        "y_val=y[train_size:]"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQHyb9yggD-9"
      },
      "source": [
        "#WE ARE USING LOF TRANSFORMED TARGET VARIABLE FOR ALL OF PUR MODELS\n",
        "# THIS CELL LOG TRANSFORMS THE DATA AND SCALES IT AS WELL\n",
        "y_train=np.log(y_train.astype(np.int))\n",
        "y_val=np.log(y_val.astype(np.int))\n",
        "y_max=max(np.max(y_train),np.max(y_val))\n",
        "y_train=y_train/y_max\n",
        "y_val=y_val/y_max"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1-ykazxEhLC"
      },
      "source": [
        "Lets also sample the data\n",
        "\n",
        "**Why do we do this??**\n",
        "\n",
        "WE ARE SAMPLING THE DATA TO INTRODUCE SPARSITY INTO THE DATA. WE HAVE VERY LESS FEATURES (7) AS COMPARED TO TOTAL SAMPLES (AROUND 10,00,000). HENCE TO INTRODUCE THE SPARSITY, WE ARE RANDOMLY SAMPLING 2,00,000 OUT OF THE WHOLE DATASET."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ednNv14aEpjS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15e8b2cd-99bd-4804-97d3-bb95483b9b01"
      },
      "source": [
        "X_train, y_train = sample(X_train, y_train, 200000)  # Simulate data sparsity\n",
        "print(\"Number of samples used for training: \" + str(y_train.shape[0]))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of samples used for training: 200000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ayLSo_1E0yF"
      },
      "source": [
        "## Now lets work with Models without embedding!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1c33g9Kjlcr"
      },
      "source": [
        "**We will be log-transforming the dependent variable(y) because it is long-tailed** - keep this in mind for each model or do the conversion after you split the data itself"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VxpJyr-Gct-"
      },
      "source": [
        "### 2.1: Random Forests\n",
        "\n",
        "1. Define a RandomForestRegressor model - with n_esitmators = 200, max_depth = 35, min_samples_split = 2, min_samples_leaf = 1\n",
        "2. Fit on the training data\n",
        "3. Predict on the validation and training data\n",
        "4. evaluate the model - calculate the MAPE for validation and training data\n",
        "\n",
        "**These parameters are from the paper** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDZmb-WyuWCd",
        "outputId": "50c05e3c-b06f-437f-9e2f-267e080c7c5f"
      },
      "source": [
        "#Here we have all the features except store index one hot encoded\n",
        "print('The shape of the training data is:',X_train.shape)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The shape of the training data is: (200000, 69)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGX-_sRYkxPv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e648f26-79e1-4a2b-c86d-cc296b101e6b"
      },
      "source": [
        "#your code here\n",
        "model=RandomForestRegressor(n_estimators = 200, max_depth = 35, min_samples_split = 2, min_samples_leaf = 1,n_jobs=-1,verbose=1)\n",
        "model.fit(X_train,y_train)\n",
        "y_pred=model.predict(X_val)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   56.8s\n",
            "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  4.1min\n",
            "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:  4.3min finished\n",
            "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.3s\n",
            "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    1.5s\n",
            "[Parallel(n_jobs=4)]: Done 200 out of 200 | elapsed:    1.5s finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQQ2SqM98ld6",
        "outputId": "7ea72cc3-4f55-4bb6-ea2f-274df1af71bf"
      },
      "source": [
        "print('MAPE score is:',MAPE(y_val,y_pred))\n",
        "print('R2 score is:',r2_score(y_val,y_pred))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MAPE score is: 0.014966925814039453\n",
            "R2 score is: 0.8054359429306465\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uv1H_lGazgx0"
      },
      "source": [
        "NOW, WE WILL ONE HOT ENCODE ALL THE FEATURES."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vHu6fPZTOqU"
      },
      "source": [
        "def hot_encode(data):\n",
        "    #your code here\n",
        "    encoder=OneHotEncoder()\n",
        "    encoder.fit(data)\n",
        "    train_data=encoder.transform(data)\n",
        "    return train_data"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XegbQhJsrvVI"
      },
      "source": [
        "train_data=hot_encode(X)\n",
        "X_train=train_data[:train_size,:]\n",
        "X_val=train_data[train_size:,:]\n",
        "y_train=y[:train_size]\n",
        "y_val=y[train_size:]"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kX8-PmB3rvHy"
      },
      "source": [
        "y_train=np.log(y_train.astype(np.int))\n",
        "y_val=np.log(y_val.astype(np.int))\n",
        "y_max=max(np.max(y_train),np.max(y_val))\n",
        "y_train=y_train/y_max\n",
        "y_val=y_val/y_max"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S__t21uFz8tR",
        "outputId": "bf95484d-1e4e-46d5-8eae-57584c727b5a"
      },
      "source": [
        "X_train, y_train = sample(X_train, y_train, 200000)  # Simulate data sparsity\n",
        "print(\"Number of samples used for training: \" + str(y_train.shape[0]))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of samples used for training: 200000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szd3QaO61OXR",
        "outputId": "b9886ba8-fe1e-4f10-f80b-f7ad777f55b4"
      },
      "source": [
        "print('The shape of the training data is:',X_train.shape)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The shape of the training data is: (200000, 1183)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtrX0b1dz8qf",
        "outputId": "6c73c165-12f7-403e-a102-5efe38ee2267"
      },
      "source": [
        "model=RandomForestRegressor(n_estimators = 200, max_depth = 35, min_samples_split = 2, min_samples_leaf = 1,n_jobs=-1,verbose=1)\n",
        "model.fit(X_train,y_train)\n",
        "y_pred=model.predict(X_val)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  1.6min\n",
            "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  7.2min\n",
            "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:  7.4min finished\n",
            "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.2s\n",
            "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.8s\n",
            "[Parallel(n_jobs=4)]: Done 200 out of 200 | elapsed:    0.8s finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFNGGpho1S4m",
        "outputId": "abed1383-cb07-4bd5-f515-20c8906202bf"
      },
      "source": [
        "print('MAPE score is:',MAPE(y_val,y_pred))\n",
        "print('R2 score is:',r2_score(y_val,y_pred))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MAPE score is: 0.026277695002020946\n",
            "R2 score is: 0.4794118407199113\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1SzeVM332NH"
      },
      "source": [
        "result={}\n",
        "result['Without EE']={'Random Forest':MAPE(y_val,y_pred)}"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KZZ271P3iMW"
      },
      "source": [
        "Here, we are noticing that the the MAPE score is increasing and the R2 score is decreasing quite a lot. I am not able to completely understand the reason behind this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcSEUp3GHC_Q"
      },
      "source": [
        "### 2.2 Boosted Trees"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Cq3sVKaHRAL"
      },
      "source": [
        "For boosting, we will use XGBoost for regression\n",
        "1. We will create a DMatrix from XGB for this - because we want to define a param_grid here. \n",
        "  * Again look at the parameters from the paper\n",
        "2. The DMatrix should be provided with X_train and label as y_train\n",
        "3. Parameters will be as follows:\n",
        "  * 'nthread': -1,\n",
        "  * 'max_depth': 7,\n",
        "  * 'eta': 0.02,\n",
        "  * 'silent': 1,\n",
        "  * 'objective': 'reg:linear',\n",
        "  * 'colsample_bytree': 0.7,\n",
        "  * 'subsample': 0.7\n",
        "  * num_round = 3000\n",
        "\n",
        "4. Train the model\n",
        "\n",
        "5. Note xgb.DMatrix needs features from Xtrain and Xval while predicting. Hence define:\n",
        "```\n",
        "feature_Xtr = xgb.DMatrix(X_train)\n",
        "feature_Xval = xgb.DMatrix(X_val)\n",
        "```\n",
        "5. Predict on feature_Xtr and feature_Xval \n",
        "6. Calculate MAPE for both\n",
        "\n",
        "\n",
        "\n",
        "Look at XGBoost [documentation](https://xgboost.readthedocs.io/en/latest/python/python_intro.html) for each parameter information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1--s_4pGOuB"
      },
      "source": [
        "#your code here\n",
        "dtrain=xgb.DMatrix(X_train,label=y_train)\n",
        "num_round=3000\n",
        "param={'nthread': -1,'max_depth': 7,'eta': 0.02,'silent': 1,'objective': 'reg:linear','colsample_bytree': 0.7,'subsample': 0.7}\n",
        "best=xgb.train(param,dtrain,num_round)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SN04NXPeyBEq"
      },
      "source": [
        "feature_Xtr = xgb.DMatrix(X_train)\n",
        "feature_Xval = xgb.DMatrix(X_val)\n",
        "ytrain_pred=best.predict(feature_Xtr)\n",
        "yval_pred=best.predict(feature_Xval)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFl8PZrayA7M",
        "outputId": "5266b3c6-bd19-478f-b7b6-e7f29031afbf"
      },
      "source": [
        "print('Train MAPE score is:',MAPE(y_train,ytrain_pred))\n",
        "print('Validation MAPE score is:',MAPE(y_val,yval_pred))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train MAPE score is: 0.019496172612389288\n",
            "Validation MAPE score is: 0.02025831575180082\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKObPC8D6EVv",
        "outputId": "401353a8-976c-4c64-935b-70f7ebb0a41c"
      },
      "source": [
        "print('Validation R2 score is:',r2_score(y_val,yval_pred))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation R2 score is: 0.6993665897072532\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osUpUUkk4P24"
      },
      "source": [
        "result['Without EE']['Gradient Boosted Tree']=MAPE(y_val,yval_pred)"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LIINAciJtWi"
      },
      "source": [
        "### 2.3 Multi Layer Perceptron"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wdjE8AgN-Ma"
      },
      "source": [
        "Define a Sequential model with the following:\n",
        "(Read the Part VI Part A Neural Networks)\n",
        "\n",
        "1. Dense Layer - 1000 neurons, keep the kernel_initializer as uniform, with activaation as relu\n",
        "2. Dense Layer - 500 neurons, keep the kernel_initializer as uniform, with activaation as relu\n",
        "3. Final dense layer with 1 neuron, and activation as sigmoid\n",
        "4. Compile the model on mean absolute error and optimizer as adam\n",
        "5. Fit the model on 10 epochs and batch size as 128, find the MAPE "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_wvmj1lNmhR"
      },
      "source": [
        "#Build the model\n",
        "#your code here\n",
        "model = Sequential()\n",
        "model.add(Dense(1000,activation='relu'))\n",
        "model.add(Dense(500,activation='relu'))\n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "model.compile(optimizer='adam', loss='mae')\n",
        "# This builds the model for the first time:"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-qcYmI5la84"
      },
      "source": [
        "#we will rescale our y_train for the model\n",
        "#the reason for this is mentioned in the paper in the same section\n",
        "# to see this change you can plot the log(Sale) vs log(Sale_max) and see how it varies\n",
        "max_log_y = max(np.max(np.log(y_train.astype(np.int))), np.max(np.log(y_val.astype(np.int))))\n",
        "fitting_y = np.log(y_train.astype(np.int)) / max_log_y\n",
        "#Since we have already done this, there is no need to do it again"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hD14nr4ik0re",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2634f611-162c-41db-f4ca-c094b03c5d58"
      },
      "source": [
        "#fit your model \n",
        "#your code here\n",
        "model.fit(X_train, y_train, batch_size=128, epochs=10)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1563/1563 [==============================] - 17s 10ms/step - loss: 0.0185\n",
            "Epoch 2/10\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.0082\n",
            "Epoch 3/10\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.0071\n",
            "Epoch 4/10\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.0064\n",
            "Epoch 5/10\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.0058\n",
            "Epoch 6/10\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.0054\n",
            "Epoch 7/10\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.0050\n",
            "Epoch 8/10\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.0047\n",
            "Epoch 9/10\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.0044\n",
            "Epoch 10/10\n",
            "1563/1563 [==============================] - 16s 10ms/step - loss: 0.0041\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1f6dff6d50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJXS4aFj7Mq6"
      },
      "source": [
        "y_pred=model.predict(X_val)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eia1YvJ27N4X",
        "outputId": "41878020-6480-4bd2-b9d9-6969d8f04ad3"
      },
      "source": [
        "print('R2 score for the Neural Net model is:',r2_score(y_val,y_pred))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "R2 score for the Neural Net model is: 0.8880116849854446\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aN0zoTVdG2MI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed1ce51c-4fa1-4e0f-969a-e52961ff7713"
      },
      "source": [
        "#predict and mape calculation\n",
        "#your code here\n",
        "y_pred=model.predict(X_val)\n",
        "print('Validation MAPE score is:',MAPE(y_val,y_pred.flatten()))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation MAPE score is: 0.011896977798439066\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTi21p4o8PjR"
      },
      "source": [
        "result['Without EE']['Neural Network']=MAPE(y_val,y_pred.flatten())"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-5celHwnaLO"
      },
      "source": [
        "# You are done with Part 2!!\n",
        "Print out the MAPE values for all models, you will need this in hand while working on Part 3 for comparing!"
      ]
    }
  ]
}