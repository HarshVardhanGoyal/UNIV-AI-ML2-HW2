{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of HW2_Part2_distribute.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Surojit-KB/UNIV-AI-ML2-HW2/blob/main/Copy_of_HW2_Part2_distribute.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaA1H2AHyVWQ"
      },
      "source": [
        "# ML-2: Trees, Model Interrogation and Bayesian Workflow\n",
        "# Homework 2: Rossman Kaggle: Forecasting Sales\n",
        "# Part 2 : Modelling without embeddings!\n",
        "**ML-2 Cohort 1** <br>\n",
        "**Instructor: Dr. Rahul Dave**<br>\n",
        "**Max Score: 100** <br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jBFtovs3nh1"
      },
      "source": [
        "#importing libraries\n",
        "import numpy as np\n",
        "import scipy.stats\n",
        "import scipy.special\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.mlab as mlab\n",
        "import seaborn as sns\n",
        "from matplotlib import cm\n",
        "import pandas as pd\n",
        "from sklearn.pipeline import make_pipeline, make_union, Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "from keras.models import Sequential\n",
        "from keras.models import Model as KerasModel\n",
        "from keras.layers import Input, Dense, Activation, Reshape\n",
        "from keras.layers import Concatenate\n",
        "from keras.layers.embeddings import Embedding\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn import linear_model\n",
        "import pickle\n",
        "import csv\n",
        "from datetime import datetime\n",
        "from sklearn import preprocessing\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import xgboost as xgb\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.metrics import r2_score\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yI5GH62Ds-XK"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2sEs45v7Rr2"
      },
      "source": [
        "## Part 2: Modelling without Entity Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uk7vU2zkGSXG"
      },
      "source": [
        "Remember the parameters we need to use\n",
        "\n",
        "![Parameters.jpeg](https://drive.google.com/uc?export=view&id=1ROfqM3F5hWwJyrvQr_J1ATovNIW5niOs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5Hi8ehR7WFX"
      },
      "source": [
        "Lets import the feature_train_data.pickle file and set X,y values from the pickle file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVnBtoigYlt1",
        "outputId": "37074cae-cd6f-46fe-eca1-ef78ddc9c94b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arl_aj6X7VQk"
      },
      "source": [
        "with open(\"/content/drive/MyDrive/Colab Notebooks/UNIV-AI-ML2/HW/feature_train_data.pickle\", 'rb') as f:\n",
        "    (X,y) = pickle.load(f)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwMxtFTUDRFN"
      },
      "source": [
        "# we will split the train_ratio and 90% and 10% and set the train_size\n",
        "train_ratio = 0.9\n",
        "num_records = len(X)\n",
        "train_size = int(train_ratio * num_records)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpP1Emg2Dni0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcd0154f-1212-42a7-9d63-54a9ebcc00ac"
      },
      "source": [
        "#lets look at our data\n",
        "X[1], y[1]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([1058,    0,    0,    0,    1,    0,    0,    1]), '4491')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2fhYw49Dz3w"
      },
      "source": [
        "The next set of inputs is following:\n",
        "\n",
        "1. Do you want to one hot encode the data?\n",
        "2. Do you want to provide embeddings as input - this will be set to True for models with entity embeddings\n",
        "3. Do you want to save the emmbeddings? - again set to true if you want to entity embeddings\n",
        "4. if 3 is set to true, we want to save them to a embeddings.pickle\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tqtv6VWWDuZu"
      },
      "source": [
        "one_hot_as_input = True #one_hot is set to True\n",
        "embeddings_as_input = False #embeddings later on needs to set to true for Part 3\n",
        "save_embeddings = False\n",
        "saved_embeddings_fname = \"embeddings.pickle\"  # set save_embeddings to True to create this file"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPSoah0_F9Ed"
      },
      "source": [
        "Define a function to one hot encode the training set and after split transform your training set using the function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KS03CrxWjcNN"
      },
      "source": [
        "def hot_encode(data):\n",
        "    #your code here\n",
        "    encoder=OneHotEncoder()\n",
        "    encoder.fit(data)\n",
        "    train_data=encoder.transform(data)\n",
        "    print(train_data.shape)\n",
        "    #train_data=np.concatenate((data[:,0].reshape(-1,1),train_data),axis=1)\n",
        "    return train_data"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCk7tyVQEdy6"
      },
      "source": [
        "Split the data into X_train, X_val, y_train, y_val based on the train_size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olO5Mvzgs54_",
        "outputId": "9c5cf350-4559-46a3-e211-c785a440a35b"
      },
      "source": [
        "train_data=hot_encode(X)\n",
        "X_train,X_val,y_train,y_val=train_test_split(train_data,y,train_size=0.9)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(844338, 1183)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLTrTSGPEaGw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "068280ea-b243-4465-92a7-6e437d608f20"
      },
      "source": [
        "#your code here\n",
        "train_data=hot_encode(X)\n",
        "X_train=train_data[:train_size,:]\n",
        "X_val=train_data[train_size:,:]\n",
        "y_train=y[:train_size]\n",
        "y_val=y[train_size:]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(844338, 1183)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQHyb9yggD-9"
      },
      "source": [
        "#y_train=np.log(y_train.astype(np.int))\n",
        "#y_val=np.log(y_val.astype(np.int))\n",
        "#y_max=max(np.max(y_train),np.max(y_val))\n",
        "#y_train=y_train/y_max\n",
        "#y_val=y_val/y_max"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1-ykazxEhLC"
      },
      "source": [
        "Lets also sample the data\n",
        "\n",
        "**Why do we do this??**\n",
        "\n",
        "your answer here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StnmulRkEnRt"
      },
      "source": [
        "def sample(X, y, n):\n",
        "    '''random samples'''\n",
        "    num_row = X.shape[0]\n",
        "    indices = np.random.randint(num_row, size=n)\n",
        "    return X[indices, :], y[indices]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ednNv14aEpjS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2c8ce24-8185-4953-f7e3-867843646f0b"
      },
      "source": [
        "X_train, y_train = sample(X_train, y_train, 200000)  # Simulate data sparsity\n",
        "print(\"Number of samples used for training: \" + str(y_train.shape[0]))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of samples used for training: 200000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ayLSo_1E0yF"
      },
      "source": [
        "## Now lets work with Models without embedding!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KT5efxrLE7Cd"
      },
      "source": [
        "**Lets define MAPE first**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRpKGtOLE6TV"
      },
      "source": [
        "#defining mape\n",
        "def MAPE(Y_actual,Y_Predicted):\n",
        "    #your code here\n",
        "    return np.mean(np.abs((Y_actual-Y_Predicted)/Y_actual))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1c33g9Kjlcr"
      },
      "source": [
        "**We will be log-transforming the dependent variable(y) because it is long-tailed** - keep this in mind for each model or do the conversion after you split the data itself"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VxpJyr-Gct-"
      },
      "source": [
        "### 2.1: Random Forests\n",
        "\n",
        "1. Define a RandomForestRegressor model - with n_esitmators = 200, max_depth = 35, min_samples_split = 2, min_samples_leaf = 1\n",
        "2. Fit on the training data\n",
        "3. Predict on the validation and training data\n",
        "4. evaluate the model - calculate the MAPE for validation and training data\n",
        "\n",
        "**These parameters are from the paper** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXzvT17zt-SH",
        "outputId": "22f71ddc-ba30-443c-a129-93840db79ce2"
      },
      "source": [
        "model=RandomForestRegressor(n_estimators = 200, max_depth = 35, min_samples_split = 2, min_samples_leaf = 1,n_jobs=-1,verbose=1)\n",
        "model.fit(X_train,y_train)\n",
        "y_pred=model.predict(X_val)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  1.6min\n",
            "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  7.2min\n",
            "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:  7.5min finished\n",
            "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.2s\n",
            "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.8s\n",
            "[Parallel(n_jobs=4)]: Done 200 out of 200 | elapsed:    0.8s finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGX-_sRYkxPv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e05df842-f751-4c7b-b0e5-238fb463ff2e"
      },
      "source": [
        "#your code here\n",
        "model=RandomForestRegressor(n_estimators = 200, max_depth = 35, min_samples_split = 2, min_samples_leaf = 1,n_jobs=-1,verbose=1)\n",
        "model.fit(X_train,y_train)\n",
        "y_pred=model.predict(X_val)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  1.7min\n",
            "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  7.4min\n",
            "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:  7.7min finished\n",
            "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.6s\n",
            "[Parallel(n_jobs=4)]: Done 200 out of 200 | elapsed:    0.6s finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQQ2SqM98ld6",
        "outputId": "ae244702-5f53-4d1c-9b72-5cae7a420813"
      },
      "source": [
        "print('MAPE score is:',MAPE(y_val,y_pred))"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MAPE score is: 0.02635512836069958\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jjidja9L2stW",
        "outputId": "03da9ed0-0561-4001-c3db-1b52badb8fdc"
      },
      "source": [
        "model.predict(X_val[45069,:])"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=4)]: Done 200 out of 200 | elapsed:    0.0s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.62269748])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bAMEmbfr21ay",
        "outputId": "3d444182-00c5-43cf-cb07-568febcaeb95"
      },
      "source": [
        "y_val[45069]"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5965278036405108"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhmsOupb2lnS",
        "outputId": "60dbb447-7b40-4eba-a0af-32669aeec2b5"
      },
      "source": [
        "r2_score(y_val,y_pred)"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.47692905962258925"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcSEUp3GHC_Q"
      },
      "source": [
        "### 2.2 Boosted Trees"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Cq3sVKaHRAL"
      },
      "source": [
        "For boosting, we will use XGBoost for regression\n",
        "1. We will create a DMatrix from XGB for this - because we want to define a param_grid here. \n",
        "  * Again look at the parameters from the paper\n",
        "2. The DMatrix should be provided with X_train and label as y_train\n",
        "3. Parameters will be as follows:\n",
        "  * 'nthread': -1,\n",
        "  * 'max_depth': 7,\n",
        "  * 'eta': 0.02,\n",
        "  * 'silent': 1,\n",
        "  * 'objective': 'reg:linear',\n",
        "  * 'colsample_bytree': 0.7,\n",
        "  * 'subsample': 0.7\n",
        "  * num_round = 3000\n",
        "\n",
        "4. Train the model\n",
        "\n",
        "5. Note xgb.DMatrix needs features from Xtrain and Xval while predicting. Hence define:\n",
        "```\n",
        "feature_Xtr = xgb.DMatrix(X_train)\n",
        "feature_Xval = xgb.DMatrix(X_val)\n",
        "```\n",
        "5. Predict on feature_Xtr and feature_Xval \n",
        "6. Calculate MAPE for both\n",
        "\n",
        "\n",
        "\n",
        "Look at XGBoost [documentation](https://xgboost.readthedocs.io/en/latest/python/python_intro.html) for each parameter information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1--s_4pGOuB"
      },
      "source": [
        "#your code here\n",
        "dtrain=xgb.DMatrix(X_train,label=y_train)\n",
        "num_round=3000\n",
        "param={'nthread': -1,'max_depth': 7,'eta': 0.02,'silent': 1,'objective': 'reg:linear','colsample_bytree': 0.7,'subsample': 0.7}\n",
        "best=xgb.train(param,dtrain,num_round)"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SN04NXPeyBEq"
      },
      "source": [
        "feature_Xtr = xgb.DMatrix(X_train)\n",
        "feature_Xval = xgb.DMatrix(X_val)\n",
        "ytrain_pred=best.predict(feature_Xtr)\n",
        "yval_pred=best.predict(feature_Xval)"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFl8PZrayA7M",
        "outputId": "25bd0535-518a-4207-fe86-1ca86666829c"
      },
      "source": [
        "print('Train MAPE score is:',MAPE(y_train,ytrain_pred))\n",
        "print('Validation MAPE score is:',MAPE(y_val,yval_pred))"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train MAPE score is: 0.03140214820682984\n",
            "Validation MAPE score is: 0.03018019840444101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LIINAciJtWi"
      },
      "source": [
        "### 2.3 Multi Layer Perceptron"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wdjE8AgN-Ma"
      },
      "source": [
        "Define a Sequential model with the following:\n",
        "(Read the Part VI Part A Neural Networks)\n",
        "\n",
        "1. Dense Layer - 1000 neurons, keep the kernel_initializer as uniform, with activaation as relu\n",
        "2. Dense Layer - 500 neurons, keep the kernel_initializer as uniform, with activaation as relu\n",
        "3. Final dense layer with 1 neuron, and activation as sigmoid\n",
        "4. Compile the model on mean absolute error and optimizer as adam\n",
        "5. Fit the model on 10 epochs and batch size as 128, find the MAPE "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_wvmj1lNmhR"
      },
      "source": [
        "#Build the model\n",
        "#your code here\n",
        "model = Sequential()\n",
        "model.add(Dense(1000,activation='relu'))\n",
        "model.add(Dense(500,activation='relu'))\n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "model.compile(optimizer='adam', loss='mae')\n",
        "# This builds the model for the first time:"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-qcYmI5la84"
      },
      "source": [
        "#we will rescale our y_train for the model\n",
        "#the reason for this is mentioned in the paper in the same section\n",
        "# to see this change you can plot the log(Sale) vs log(Sale_max) and see how it varies\n",
        "max_log_y = max(np.max(np.log(y_train.astype(np.int))), np.max(np.log(y_val.astype(np.int))))\n",
        "fitting_y = np.log(y_train.astype(np.int)) / max_log_y"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hD14nr4ik0re",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0a1fcf9-a025-4ce4-f8d2-f46c195d8baf"
      },
      "source": [
        "#fit your model \n",
        "#your code here\n",
        "model.fit(X_train, fitting_y, batch_size=128, epochs=10)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1563/1563 [==============================] - 19s 11ms/step - loss: 0.0184\n",
            "Epoch 2/10\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 0.0082\n",
            "Epoch 3/10\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 0.0072\n",
            "Epoch 4/10\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 0.0064\n",
            "Epoch 5/10\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 0.0059\n",
            "Epoch 6/10\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 0.0054\n",
            "Epoch 7/10\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 0.0050\n",
            "Epoch 8/10\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 0.0047\n",
            "Epoch 9/10\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 0.0044\n",
            "Epoch 10/10\n",
            "1563/1563 [==============================] - 18s 11ms/step - loss: 0.0042\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fa1b36e7310>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aN0zoTVdG2MI"
      },
      "source": [
        "#predict and mape calculation\n",
        "#your code here\n",
        "y_pred=model.predict(X_val)\n",
        "y_val=np.log(y_val.astype(np.int)) / max_log_y\n",
        "print('Validation MAPE score is:',MAPE(y_val,y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-5celHwnaLO"
      },
      "source": [
        "# You are done with Part 2!!\n",
        "Print out the MAPE values for all models, you will need this in hand while working on Part 3 for comparing!"
      ]
    }
  ]
}