{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Hw2_Part4_distribute.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Surojit-KB/UNIV-AI-ML2-HW2/blob/main/Copy_of_Hw2_Part4_distribute.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaA1H2AHyVWQ"
      },
      "source": [
        "# ML-2: Trees, Model Interrogation and Bayesian Workflow\n",
        "# Homework 2: Rossman Kaggle: Forecasting Sales\n",
        "# Part 4 : Modelling with embeddings!\n",
        "**ML-2 Cohort 1** <br>\n",
        "**Instructor: Dr. Rahul Dave**<br>\n",
        "**Max Score: 100** <br>\n",
        "Name of Students:\n",
        "1. Sukriti Paul\n",
        "2. Harsh Goyal\n",
        "3. Surojit Bhattacharyya"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jBFtovs3nh1"
      },
      "source": [
        "#importing libraries\n",
        "import numpy as np\n",
        "import scipy.stats\n",
        "import scipy.special\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.mlab as mlab\n",
        "from matplotlib import cm\n",
        "import pandas as pd\n",
        "from sklearn.pipeline import make_pipeline, make_union, Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "from keras.models import Sequential\n",
        "from keras.models import Model as KerasModel\n",
        "from keras.layers import Input, Dense, Activation, Reshape\n",
        "from keras.layers import Concatenate\n",
        "from keras.layers.embeddings import Embedding\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn import linear_model\n",
        "import pickle\n",
        "import csv\n",
        "from datetime import datetime\n",
        "from sklearn import preprocessing\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import xgboost as xgb\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.metrics import r2_score\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlux5Gn-n-Gz"
      },
      "source": [
        "We will repeat the first initial steps again from Part 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5Hi8ehR7WFX"
      },
      "source": [
        "Lets import the feature_train_data.pickle file and set X,y values from the pickle file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XiYYuQDWUfpB",
        "outputId": "a9afa46b-d2d0-4f43-d8a6-14d597cb508a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arl_aj6X7VQk"
      },
      "source": [
        "#your code here\n",
        "with open(\"/content/drive/MyDrive/Colab Notebooks/UNIV-AI-ML2/HW/feature_train_data.pickle\", 'rb') as f:\n",
        "    (X,y) = pickle.load(f)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aqsBgYbDQYD"
      },
      "source": [
        "with open(\"/content/drive/MyDrive/Colab Notebooks/UNIV-AI-ML2/HW/result.pickle\", 'rb') as f:\n",
        "    result = pickle.load(f)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwMxtFTUDRFN"
      },
      "source": [
        "# we will split the train_ratio and 90% and 10% and set the train_size\n",
        "train_ratio = 0.9\n",
        "num_records = len(X)\n",
        "train_size = int(train_ratio * num_records)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpP1Emg2Dni0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "214720eb-9713-43b9-a47a-223cda1331f6"
      },
      "source": [
        "#lets look at our data\n",
        "X[1], y[1]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([1058,    0,    0,    0,    1,    0,    0,    1]), '4491')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2fhYw49Dz3w"
      },
      "source": [
        "The next set of inputs is following: Write the code for this yourself\n",
        "\n",
        "1. Do you want to one hot encode the data?\n",
        "2. Do you want to provide embeddings as input - this will be set to True for models with entity embeddings\n",
        "3. Do you want to save the emmbeddings? - again set to true if you want to entity embeddings\n",
        "4. if 3 is set to true, we want to save them to a embeddings.pickle\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tqtv6VWWDuZu"
      },
      "source": [
        "#your code here\n",
        "one_hot_as_input = False \n",
        "embeddings_as_input = True \n",
        "save_embeddings = True\n",
        "saved_embeddings_fname = \"embeddings.pickle\" "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ayLSo_1E0yF"
      },
      "source": [
        "## Now lets work with Models with Entity embedding!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTYK-isysXiu"
      },
      "source": [
        "Now that you have saved embeddings - push this into the other models as an input with X. \n",
        "\n",
        "How will we do this? \n",
        "\n",
        "We need to update our X values. \n",
        "\n",
        "1. We will define a function called embed_features, which will combine the embedding with X. \n",
        "2. Call this function and update it with the inital X values taken from the pickle file - features_train_data\n",
        "3. Then split you data, X_emb - into Xtrain and X_Val, y_train and y_val remain the same\n",
        "4. Sample the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbymoNn65ETi"
      },
      "source": [
        "#call our saved embeddings from part 3\n",
        "saved_embeddings_fname = \"/content/drive/MyDrive/Colab Notebooks/UNIV-AI-ML2/HW/embeddings.pickle\""
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRyCLdbIV8_-"
      },
      "source": [
        "f_embeddings = open(saved_embeddings_fname, \"rb\")\n",
        "embeddings = pickle.load(f_embeddings)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJjSG5BTW0-F",
        "outputId": "39ecd35c-5473-421e-b43b-2348cbd93861"
      },
      "source": [
        "for embed in embeddings:\n",
        "  print(embed.shape)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1115, 10)\n",
            "(7, 6)\n",
            "(3, 2)\n",
            "(12, 6)\n",
            "(31, 10)\n",
            "(12, 6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDegrAuAYhHo"
      },
      "source": [
        "data_order=['store','year','month','day','dow','open','promo','state']\n",
        "embeddings_order=['store','dow','year','month','day','state']"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUVh51hgqCyp"
      },
      "source": [
        "#combining embedding\n",
        "def embed_features(X, saved_embeddings_fname):\n",
        "    f_embeddings = open(saved_embeddings_fname, \"rb\")\n",
        "    embeddings = pickle.load(f_embeddings)\n",
        "\n",
        "    index_embedding_mapping = {0: 0, 1: 2, 2: 3, 3: 4, 4: 1, 7: 5}\n",
        "    X_embedded = []\n",
        "\n",
        "    (num_records, num_features) = X.shape\n",
        "    for record in X:\n",
        "        embedded_features = []\n",
        "        for i, feat in enumerate(record):\n",
        "            feat = int(feat)\n",
        "            if i not in index_embedding_mapping.keys():\n",
        "                embedded_features += [feat]\n",
        "            else:\n",
        "                embedding_index = index_embedding_mapping[i]\n",
        "                embedded_features += embeddings[embedding_index][feat].tolist()\n",
        "\n",
        "        X_embedded.append(embedded_features)\n",
        "\n",
        "    return np.array(X_embedded)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hf8U7znjHZrW"
      },
      "source": [
        "**Explain what is happening the function above**\n",
        "\n",
        "Here we are replacing our data values by there respective embeddings. If we take an embedding, for example, the Store index embedding, it has a shape of (1115,10) which means for each of the 1115 store indexes, it has a vector of 10 as the embedding. This is the vector that we call as the embedding and we want to use it to replace the corresponding data value. So, we are first going to the correct feature embedding by giving the embedding index correctly and then going to the correct data value by providing the data value as the further index and converting the embedding to list and setting it as the data value.<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALCfACl0rzeK"
      },
      "source": [
        "### Embedding with X - input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wcLbyHps8_W"
      },
      "source": [
        "#check if embedding is needed, if yes call embed_features - with X and the embeddings passed to it - call this new X as X_emb\n",
        "if embeddings_as_input:\n",
        "  #your code here \n",
        "  X_emb=embed_features(X,saved_embeddings_fname)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDzhYvCJHm8d"
      },
      "source": [
        "Split the train and validation based on train size and on the new X_emb values from the previous code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLTrTSGPEaGw"
      },
      "source": [
        "#update the X_train and X_val\n",
        "#your code here \n",
        "X_train=X_emb[:train_size,:]\n",
        "X_val=X_emb[train_size:,:]\n",
        "y_train=y[:train_size]\n",
        "y_train=y_train.astype(np.int)\n",
        "y_val=y[train_size:]\n",
        "y_val=y_val.astype(np.int)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StnmulRkEnRt"
      },
      "source": [
        "def sample(X, y, n):\n",
        "    '''random samples'''\n",
        "    num_row = X.shape[0]\n",
        "    indices = np.random.randint(num_row, size=n)\n",
        "    return X[indices, :], y[indices]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ednNv14aEpjS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6ec719d-8b5b-4383-dd24-52ae4e65f68f"
      },
      "source": [
        "X_train, y_train = sample(X_train, y_train, 200000)  # Simulate data sparsity\n",
        "print(\"Number of samples used for training: \" + str(y_train.shape[0]))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of samples used for training: 200000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1I6AT5MuRTH"
      },
      "source": [
        "## Add the embeddings into the Models and check their MAPE!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmJYNA0PH5AU"
      },
      "source": [
        "All the models defined here will have the same parameters as the ones defined in Part 2!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTRegclpugs3"
      },
      "source": [
        "#define MAPE Function here\n",
        "#your code here\n",
        "def MAPE(Y_actual,Y_Predicted):\n",
        "    #your code here\n",
        "    return np.mean(np.abs((Y_actual-Y_Predicted)/Y_actual))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4Sjp7NQu1ow"
      },
      "source": [
        "### Model 1: Random Forests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdCkr0Y2b1Fs"
      },
      "source": [
        "#LOG TRANSFORMING Y VALUES AND THEN SCALING THEM\n",
        "y_train=np.log(y_train)\n",
        "y_val=np.log(y_val)\n",
        "y_max=max(np.max(y_train),np.max(y_val))\n",
        "y_train=y_train/y_max\n",
        "y_val=y_val/y_max"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7QVgZezGAOq",
        "outputId": "610b3982-9a6b-4153-d7c4-b8cc495809c2"
      },
      "source": [
        "#Using log scaled target values \n",
        "model1=RandomForestRegressor(n_estimators = 200, max_depth = 35, min_samples_split = 2, min_samples_leaf = 1,n_jobs=-1,verbose=1)\n",
        "model1.fit(X_train,y_train)\n",
        "y_pred=model1.predict(X_val)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  1.1min\n",
            "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  4.8min\n",
            "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:  5.0min finished\n",
            "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.4s\n",
            "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    1.9s\n",
            "[Parallel(n_jobs=4)]: Done 200 out of 200 | elapsed:    2.0s finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5ecCKSrGAEC",
        "outputId": "73965601-bd58-4066-9ef7-d037183b7859"
      },
      "source": [
        "print('MAPE score is:',MAPE(y_val,y_pred))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MAPE score is: 0.012296595383149252\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwBbI8rFFFzi"
      },
      "source": [
        "result['With EE']['Random Forest']=MAPE(y_val,y_pred)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYvVmYMpu_Vi"
      },
      "source": [
        "### Model 2: Boosting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1--s_4pGOuB"
      },
      "source": [
        "#your code here\n",
        "dtrain=xgb.DMatrix(X_train,label=y_train)\n",
        "num_round=3000\n",
        "param={'nthread': -1,'max_depth': 7,'eta': 0.02,'silent': 1,'objective': 'reg:linear','colsample_bytree': 0.7,'subsample': 0.7}\n",
        "best=xgb.train(param,dtrain,num_round)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acTnU6QBcSbr"
      },
      "source": [
        "feature_Xtr = xgb.DMatrix(X_train)\n",
        "feature_Xval = xgb.DMatrix(X_val)\n",
        "ytrain_pred=best.predict(feature_Xtr)\n",
        "yval_pred=best.predict(feature_Xval)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "St2L-h_OcWBT",
        "outputId": "faf23bc7-c778-4b93-d740-766318ff46d7"
      },
      "source": [
        "print('Train MAPE score is:',MAPE(y_train,ytrain_pred))\n",
        "print('Validation MAPE score is:',MAPE(y_val,yval_pred))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train MAPE score is: 0.0063567874485113635\n",
            "Validation MAPE score is: 0.01289825014905645\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6d46Ul5GSI7"
      },
      "source": [
        "result['With EE']['Gradient Boosted Tree']=MAPE(y_val,yval_pred)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "oiMT0PsbOw01",
        "outputId": "6aeaad88-9ed3-4b12-e06a-5470c95db49d"
      },
      "source": [
        "df=pd.DataFrame.from_records(result)\n",
        "df"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>With EE</th>\n",
              "      <th>Without EE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Neural Network</th>\n",
              "      <td>0.011585</td>\n",
              "      <td>0.011897</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Random Forest</th>\n",
              "      <td>0.012297</td>\n",
              "      <td>0.026278</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Gradient Boosted Tree</th>\n",
              "      <td>0.012898</td>\n",
              "      <td>0.020258</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                        With EE  Without EE\n",
              "Neural Network         0.011585    0.011897\n",
              "Random Forest          0.012297    0.026278\n",
              "Gradient Boosted Tree  0.012898    0.020258"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "RqE7cCZiThpy",
        "outputId": "f53ffc22-6015-4051-cb3c-0896c744b88f"
      },
      "source": [
        "#Lets convert the dataframe to makrdown to show it in the final result\n",
        "df.to_markdown()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'|                       |   With EE |   Without EE |\\n|:----------------------|----------:|-------------:|\\n| Neural Network        | 0.0115853 |    0.011897  |\\n| Random Forest         | 0.0122966 |    0.0262777 |\\n| Gradient Boosted Tree | 0.0128983 |    0.0202583 |'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uh2Zl3OVPY7z"
      },
      "source": [
        "with open(\"/content/drive/MyDrive/Colab Notebooks/UNIV-AI-ML2/HW/result.pickle\", 'wb') as f:\n",
        "    pickle.dump((result),f)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhM6IVJJv0rU"
      },
      "source": [
        "## Final Commments!\n",
        "\n",
        "Apart from how long this homework was, lets make some other final comments\n",
        "\n",
        "**Question 1: Did you models with Entity Embeddings perfom better?**\n",
        "\n",
        "ANS: Yes the models with Entity Embeddings performed better than the models which were provided with the Dataset directly. We can see that in the table above, all the models have a lower MAPE score when provided with embeddings as input. Specifically, random forest, whcih was the worst perfoming model amongst the models without EE performs reasonably well( even better than Gradient Boosted Trees) when provided with the embeddings. Neural Nets have a very slight improvement but the other two models show a clear decrease in the MAPE score.\n",
        "\n",
        "**Question 2: Now that you have completed this homework, lets answer the main purpose of the homework - How do you think entity embeddings improved the MAPE score. To show this do a similar table like the one did in Paper**\n",
        "\n",
        "ANS: \n",
        "\n",
        "\n",
        "```\n",
        "|       method          |   With EE |   Without EE |\n",
        "|:----------------------|----------:|-------------:|\n",
        "| Neural Network        | 0.0115853 |    0.011897  |\n",
        "| Random Forest         | 0.0122966 |    0.0262777 |\n",
        "| Gradient Boosted Tree | 0.0128983 |    0.0202583 |\n",
        "```\n",
        "We can see the change in the MAPE scores for both the ensemble models quite clearly. Random Forest especially looks like the big winner in the EE case with a quite large change in the MAPE score. Actually, we had got better MAPE score for Random Forest without the use of one-hot encoding, but we have used the result from the one-hot encoded dataset here to keep the uniformity between all the three models. Also, we have used log-scaled target variable(*Sales*) in all the the three models.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Question 3: Add a table here to show the similar to the one done in paper - to show the different MAPE values with and without embeddings**\n",
        "\n",
        "ANS:\n",
        " \n",
        "\n",
        "\n",
        "```\n",
        "|       method          |   With EE |   Without EE |\n",
        "|:----------------------|----------:|-------------:|\n",
        "| Neural Network        | 0.0115853 |    0.011897  |\n",
        "| Random Forest         | 0.0122966 |    0.0262777 |\n",
        "| Gradient Boosted Tree | 0.0128983 |    0.0202583 |\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7I74muVVSnS"
      },
      "source": [
        "THE END<br>\n",
        "Thanks for the read."
      ]
    }
  ]
}